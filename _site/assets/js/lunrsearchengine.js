
var documents = [{
    "id": 0,
    "url": "https://prateekcodes.dev/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://prateekcodes.dev/about",
    "title": "Prateek Codes Blog",
    "body": "&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; "
    }, {
    "id": 2,
    "url": "https://prateekcodes.dev/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "https://prateekcodes.dev/",
    "title": "Home",
    "body": "      All Posts:                                                                                                                       Rails 8 introduces Parameters#expect for safer parameter handling:         Rails 8 adds `Parameters#expect` to prevent parameter manipulation attacks and provide clearer error handling when required parameters are missing or malformed. :                                                                                                     Prateek Choudhary                    30 Jun 2025                                                                                                                    HTTP Caching for Rails APIs: The Missing Performance Layer:         Most Rails APIs ignore HTTP caching entirely, missing out on massive performance gains. Learn practical caching strategies that can reduce server load by 90%. :                                                                                                     Prateek Choudhary                    29 Jun 2025                                                                                                                    Why your Rails performance fixes don't work (and how to find ones that do):         Stop wasting time optimizing code that doesn't matter. Learn how to use the Pareto Principle and profiling tools to find the 20% of your Rails code causing 80% of performance. . . :                                                                                                     Prateek Choudhary                    28 Jun 2025                                                                                                                    What are the new Rails 8 framework defaults?:         Rails 8 framework defaults include timezone preservation in to_time methods, strict HTTP freshness checking following RFC standards, and default regex timeout for DoS protection. Learn what each default does and. . . :                                                                                                     Prateek Choudhary                    28 Jun 2025                                                                                                                    Scaling Rails with PostgreSQL Read Replicas: Part 3 - Production Excellence:         Master production deployment strategies, monitoring, performance optimization, and failure handling for Rails applications using PostgreSQL read replicas. :                                                                                                     Prateek Choudhary                    25 Jun 2025                                                                                                                    Scaling Rails with PostgreSQL Read Replicas: Part 2 - Advanced Patterns and Gotchas:         Deep dive into handling replication lag, implementing automatic connection switching, and solving real-world challenges with read replicas in Rails applications. :                                                                                                     Prateek Choudhary                    25 Jun 2025                                                                                                                    Scaling Rails with PostgreSQL Read Replicas: Part 1 - Understanding the Basics:         Learn when and why to use read replicas in Rails applications, understand the architecture, and implement basic read/write splitting with real-world examples. :                                                                                                     Prateek Choudhary                    25 Jun 2025                                                                                                                    PostgreSQL 17's MERGE with RETURNING: The Game-Changer Rails Developers Have Been Waiting For:         PostgreSQL 17 introduces RETURNING support to the MERGE statement, solving a long-standing limitation that forced developers to choose between atomic upserts and knowing what actually happened to their data. :                                                                                                     Prateek Choudhary                    25 Jun 2025                                                         &laquo; Prev       1        2      Next &raquo; "
    }, {
    "id": 4,
    "url": "https://prateekcodes.dev/projects",
    "title": "Projects",
    "body": ""
    }, {
    "id": 5,
    "url": "https://prateekcodes.dev/page2/",
    "title": "Home",
    "body": "      All Posts:         {% for post in paginator. posts %}      {% assign post_date = post. date | date: '%s' %}      {% assign current_date = 'now' | date: '%s' %}      {% if post_date &lt;= current_date %}        {% include post-card. html %}      {% endif %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 6,
    "url": "https://prateekcodes.dev/rails-8-serialized-attributes-comparable-option/",
    "title": "Rails 8 adds comparable option to serialized attributes",
    "body": "2025/07/04 - Serialized attributes in Rails have always had a subtle problem: changes in how data is serialized can trigger database updates even when the actual data hasn’t changed. Rails 8 introduces the comparable option to solve this issue. The Problem: When using serialized attributes, the same data can have different string representations: 123456789101112class User &lt; ApplicationRecord serialize :preferences, coder: JSONenduser = User. create!(preferences: { theme:  dark , notifications: true })# Later, the preferences hash might be reordereduser. preferences # =&gt; { notifications  =&gt; true,  theme  =&gt;  dark }# Even though the data is the same, Rails thinks it changeduser. changed? # =&gt; trueuser. save! # Unnecessary database write!This problem becomes worse when:  JSON libraries change their serialization behavior Hash keys get reordered (Ruby doesn’t guarantee hash ordering across versions) Float precision changes slightly Whitespace or formatting differsRails 8 Solution: Rails 8 adds a comparable option that compares the deserialized objects instead of their serialized strings: 123456789101112class User &lt; ApplicationRecord serialize :preferences, coder: JSON, comparable: trueenduser = User. create!(preferences: { theme:  dark , notifications: true })# Even if the serialized form changesuser. preferences = {  notifications  =&gt; true,  theme  =&gt;  dark  }# Rails now knows the data is the sameuser. changed? # =&gt; falseuser. save! # No database write!Custom Serializers: The comparable option works with custom serializers too: 1234567891011121314class CompressedJSON def self. dump(obj)  Zlib::Deflate. deflate(obj. to_json) end  def self. load(data)  return {} if data. nil?  JSON. parse(Zlib::Inflate. inflate(data)) endendclass Archive &lt; ApplicationRecord serialize :data, coder: CompressedJSON, comparable: trueendWhen to Use comparable: true: Use it when:  Data comes from external APIs that might reorder fields Using JSON/YAML serialization where formatting can vary Storing configuration or settings that rarely change Working with legacy data that might have inconsistent serializationDon’t use it when:  The serialized data includes timestamps or unique identifiers that should trigger updates You need to track any change in serialization format Using custom serializers with complex comparison logicConclusion: The comparable option is a small but impactful addition that prevents phantom updates in Rails applications. It’s especially valuable for applications that sync data with external sources or deal with serialized configurations. References:  Pull Request #53946 introducing comparable option ActiveRecord Serialization Documentation Rails 8 Release Notes"
    }, {
    "id": 7,
    "url": "https://prateekcodes.dev/rails-8-multiple-rate-limits-per-controller/",
    "title": "Rails 8 adds ability to use multiple rate limits per controller",
    "body": "2025/07/04 - Rails 7. 2 introduced built-in rate limiting to Action Controller. However, you could only set one rate limit per controller, which wasn’t flexible enough for real-world applications. Rails 8 solves this by allowing multiple rate limits using the name: parameter. Before Rails 8: In Rails 7. 2, you were limited to a single rate limit configuration: 12345678910111213141516class PostsController &lt; ApplicationController # Only one rate limit for the entire controller rate_limit to: 10, within: 1. minute, only: :create def index  # No rate limiting end def create  # Limited to 10 requests per minute end def destroy  # No rate limiting (but what if we wanted stricter limits here?) endendThis rate_limit call means: “Allow a maximum of 10 requests within 1 minute for the create action only. ” Rails 8: Multiple Rate Limits: Now you can define different rate limiting strategies: 123456789101112131415161718192021222324class PostsController &lt; ApplicationController # Short-term: Prevent burst traffic rate_limit to: 5, within: 2. seconds, only: :create, name:  burst_control  # Long-term: Enforce hourly quotas rate_limit to: 100, within: 1. hour, only: :create, name:  hourly_limit  # Stricter limits for destructive actions rate_limit to: 10, within: 10. minutes, only: :destroy, name:  delete_limit  def index  # No rate limiting applied end def create  # Two rate limits applied:  # 1. Maximum 5 requests every 2 seconds (burst control)  # 2. Maximum 100 requests per hour (quota) end def destroy  # One rate limit: Maximum 10 deletes every 10 minutes endendLet’s break down what each rate_limit does:    Burst Control:   1rate_limit to: 5, within: 2. seconds, only: :create, name:  burst_control        Prevents users from hammering the create endpoint   If someone sends 6 requests in 2 seconds, the 6th request gets blocked      Hourly Quota:   1rate_limit to: 100, within: 1. hour, only: :create, name:  hourly_limit        Enforces a reasonable usage limit per hour   Even if requests are spread out, after 100 creates in an hour, further requests are blocked      Delete Protection:   1rate_limit to: 10, within: 10. minutes, only: :destroy, name:  delete_limit        Prevents mass deletion attacks   Allows only 10 delete operations every 10 minutes   Practical Example: API Controller: Here’s how you might protect different API endpoints with appropriate rate limits: 123456789101112131415161718192021222324class ApiController &lt; ApplicationController # Prevent rapid-fire requests rate_limit to: 10, within: 1. minute, only: :search, name:  search_burst  # Daily search quota rate_limit to: 1000, within: 1. day, only: :search, name:  search_daily  # Stricter limits for expensive operations rate_limit to: 5, within: 1. hour, only: :generate_report, name:  report_limit  def search  # Both rate limits apply here:  # - Max 10 searches per minute  # - Max 1000 searches per day end def generate_report  # Limited to 5 reports per hour end def show  # No rate limiting endendEach action can have multiple rate limits that work together. For the search action:  Users can search up to 10 times per minute (prevents abuse) But also can’t exceed 1000 searches per day (enforces fair usage)Why This Matters: Before Rails 8, if you wanted different rate limits for different actions, you had two options:  Create separate controllers (unnecessary complexity) Use external gems like rack-attack (additional dependency)Now, you can define exactly the rate limiting strategy you need, right in your controller. The Technical Fix: The problem was that all rate limits shared the same cache key. Rails 8 fixes this by including the name in the cache key: 1234567# Before (Rails 7. 2): Same key for all rate limits rate_limit:posts_controller:127. 0. 0. 1 # After (Rails 8): Unique key for each named rate limit rate_limit:posts_controller:burst_control:127. 0. 0. 1  rate_limit:posts_controller:hourly_limit:127. 0. 0. 1  rate_limit:posts_controller:delete_limit:127. 0. 0. 1 Conclusion: Rails 8’s multiple rate limits feature gives you fine-grained control over request throttling without external gems. The name: parameter is all you need to implement sophisticated rate limiting strategies. References:  PR #52960 - Add ability to use multiple rate limits per controller Issue #52957 - Multiple rate-limits share same cache key ActionController::RateLimiting Documentation"
    }, {
    "id": 8,
    "url": "https://prateekcodes.dev/rails-8-introduces-params-expect-for-safer-parameter-handling/",
    "title": "Rails 8 introduces Parameters#expect for safer parameter handling",
    "body": "2025/06/30 - Strong Parameters have been a Rails security staple since Rails 4, but they had a vulnerability: carefully crafted parameters could trigger 500 errors instead of the expected 400 Bad Request, potentially exposing application internals. Before: Previously, handling nested parameters with permit could be exploited: 1234567class UsersController &lt; ApplicationController def create  user_params = params. require(:user). permit(:name, :email, tags: [])  @user = User. create!(user_params)  render json: @user endendAn attacker could send malformed parameters to trigger a 500 error: 12345678910111213141516# Expected usagecurl -X POST http://localhost:3000/users \ -H  Content-Type: application/json  \ -d '{ user : { name :  Alice ,  tags : [ ruby ,  rails ]}}'# Malicious request causing 500 errorcurl -X POST http://localhost:3000/users \ -H  Content-Type: application/json  \ -d '{ user : { name :  Alice ,  tags :  not-an-array }}'# =&gt; 500 Internal Server Error (ActionController::UnpermittedParameters)# Another attack vectorcurl -X POST http://localhost:3000/users \ -H  Content-Type: application/json  \ -d '{ user : { name :  Alice ,  tags : { 0 :  ruby ,  1 :  rails }}}'# =&gt; 500 Internal Server ErrorThese 500 errors could reveal stack traces in development or trigger unnecessary error alerts in production. Rails 8: Rails 8 introduces Parameters#expect which validates parameter structure and returns 400 Bad Request for malformed input: 1234567class UsersController &lt; ApplicationController def create  user_params = params. expect(user: [:name, :email, tags: []])  @user = User. create!(user_params)  render json: @user endendNow the same malicious requests raise ActionController::ParameterMissing which Rails handles as a 400 Bad Request: 1234567891011# Malicious request now returns 400curl -X POST http://localhost:3000/users \ -H  Content-Type: application/json  \ -d '{ user : { name :  Alice ,  tags :  not-an-array }}'# =&gt; 400 Bad Request# Hash instead of array also caughtcurl -X POST http://localhost:3000/users \ -H  Content-Type: application/json  \ -d '{ user : { name :  Alice ,  tags : { 0 :  ruby }}}'# =&gt; 400 Bad RequestComplex Nested Parameters: expect really shines with deeply nested structures: 12345678910111213141516class ProjectsController &lt; ApplicationController def create  # Define expected structure with nested arrays  project_params = params. expect(   project: [    :name,    :description,    { settings: [:theme, :notifications] },    { team_members: [[:name, :role, permissions: []]] }   ]  )    @project = Project. create!(project_params)  render json: @project endendValid request: 12345678910111213141516171819202122{  project : {   name :  New App ,   description :  Rails application ,   settings : {    theme :  dark ,    notifications : true  },   team_members : [   {     name :  Alice ,     role :  developer ,     permissions : [ read ,  write ]   },   {     name :  Bob ,     role :  designer ,     permissions : [ read ]   }  ] }}Conclusion: Parameters#expect is a small but important security improvement. It transforms potential 500 errors into proper 400 responses, making Rails APIs more robust against parameter manipulation attacks while providing clearer feedback to API consumers. References:  Pull Request #51674 introducing Parameters#expect Rails 8 Security Guide ActionController::Parameters API"
    }, {
    "id": 9,
    "url": "https://prateekcodes.dev/rails-http-caching-strategies/",
    "title": "HTTP Caching for Rails APIs: The Missing Performance Layer",
    "body": "2025/06/29 - Every Rails developer knows the caching dance. We’ve all implemented fragment caching, played with Rails. cache, and maybe even ventured into Russian doll caching. But here’s what I’ve seen being completely ignored: HTTP caching. Here’s the thing - HTTP caching can eliminate up to 90% of your API requests without you writing a single line of caching logic. It’s built into every HTTP client worth using, requires zero infrastructure, and costs nothing to implement. Yet most Rails APIs serve every request fresh, ignoring decades of HTTP specification designed specifically to solve this problem. The Hidden Cost of Ignoring HTTP Caching: Let’s start with a typical Rails API endpoint: 123456class Api::ProductsController &lt; ApplicationController def show  @product = Product. find(params[:id])  render json: @product endendEvery request to this endpoint:  Hits your Rails server Queries your database Serializes the response Consumes server resourcesEven if the product hasn’t changed in months. Now imagine this endpoint serves a mobile app with 100,000 daily active users, each checking product details multiple times per day. That’s millions of unnecessary requests, database queries, and server cycles. Rails’ CSRF Problem (And Why APIs Don’t Have It): Before diving into solutions, let’s address why HTTP caching is rarely discussed in Rails circles. Rails applications typically embed CSRF tokens in every HTML response: 1&lt;meta name= csrf-token  content= Xc0vf6L7hgb. . .   /&gt;This token changes on every request, making HTML responses effectively uncacheable. But APIs don’t have this problem - they typically use token-based authentication without CSRF protection. This makes APIs the perfect candidate for aggressive HTTP caching strategies. Hello Cache-Control: The Cache-Control header is where the magic happens. It tells clients and CDNs exactly how to cache your responses. Here’s what a properly cached API response looks like: 12345678class Api::ProductsController &lt; ApplicationController def show  @product = Product. find(params[:id])  response. headers['Cache-Control'] = 'public, max-age=3600'  render json: @product endendThis simple header tells clients to cache the response for one hour (3600 seconds). During that hour, the client won’t make another request - it’ll serve the cached version instead. But we can do better. Conditional Requests: The Smart Way to Cache: What happens after that hour expires? The client makes a new request and we start over? Not quite. This is where conditional requests shine. Using Last-Modified: 123456789class Api::ProductsController &lt; ApplicationController def show  @product = Product. find(params[:id])  if stale?(last_modified: @product. updated_at)   render json: @product  end endendThe stale? method is Rails magic that checks if the resource has been modified since the client last fetched it. It automatically sets the appropriate headers and returns false if the content is fresh (triggering a 304 response). Here’s what happens:  First request: Client receives the product with a Last-Modified header Subsequent requests: Client sends If-Modified-Since header If product hasn’t changed: Rails returns 304 Not Modified (no body) If product has changed: Rails returns the full responseThe beauty? When nothing changes, you save:  JSON serialization time Response body bandwidth Client parsing timeUsing ETags for More Complex Scenarios: Sometimes updated_at isn’t enough. Maybe your response includes associated data or computed fields: 123456789101112131415161718192021class Api::ProductsController &lt; ApplicationController def show  @product = Product. find(params[:id])  # ETag based on product and its associations  etag = [   @product,   @product. reviews. maximum(:updated_at),   @product. current_price  ]  if stale?(etag: etag)   render json: {    product: @product,    review_count: @product. reviews. count,    average_rating: @product. reviews. average(:rating),    current_price: @product. current_price   }  end endendRails automatically generates an ETag from the array, creating a unique fingerprint for this exact response state. Advanced Patterns for Real-World APIs: Pattern 1: Efficient Collection Caching: Caching collections requires thinking about what actually changes: 1234567891011121314151617181920class Api::ProductsController &lt; ApplicationController def index  @products = Product. active. includes(:category)  # Use the most recent update as the collection's last modified time  last_modified = @products. maximum(:updated_at)  # Include collection  fingerprint  in ETag  etag_components = [   last_modified,   @products. count,   params[:page],   params[:per_page]  ]  if stale?(last_modified: last_modified, etag: etag_components)   render json: @products  end endendPattern 2: User-Specific Caching: Private data needs private caching: 123456789101112class Api::OrdersController &lt; ApplicationController def index  @orders = current_user. orders. recent  # Private ensures CDNs don't cache user-specific data  response. headers['Cache-Control'] = 'private, max-age=300'  if stale?(last_modified: @orders. maximum(:updated_at))   render json: @orders  end endendPattern 3: Preventing Unnecessary Queries: The real power comes from avoiding database queries entirely: 12345678910111213141516class Api::TimelineController &lt; ApplicationController def show  # Only check if we need to regenerate  latest_update = current_user. posts. maximum(:updated_at)  if stale?(last_modified: latest_update)   # Only now do we load the actual data   @posts = current_user. posts             . includes(:comments, :likes)             . order(created_at: :desc)             . limit(50)   render json: @posts  end endendThe maximum(:updated_at) query is lightning fast compared to loading full records. Cache-Control Directives That Actually Matter: While the HTTP spec defines many cache directives, here are the ones that actually matter for Rails APIs: max-age=seconds - How long to cache before checking again 1'public, max-age=3600' # Cache for 1 hourprivate vs public - Who can cache this 12'private, max-age=300' # Only browser can cache (user data)'public, max-age=3600' # CDNs can cache too (public data)no-store - Never cache this 1'no-store' # For sensitive data like payment infomust-revalidate - Always check when stale 1'public, max-age=3600, must-revalidate' # Don't serve stale contentReal-World Implementation Strategy: Step 1: Identify Cacheable Endpoints: Start with read-heavy, public endpoints:  Product catalogs Blog posts / articles Category listings Static configurationStep 2: Add Conditional Caching: 12345678910class ApplicationController &lt; ActionController::API # Helper for consistent caching def cache_publicly(max_age: 1. hour)  response. headers['Cache-Control'] =  public, max-age=#{max_age}  end def cache_privately(max_age: 5. minutes)  response. headers['Cache-Control'] =  private, max-age=#{max_age}  endendStep 3: Monitor and Iterate: Track your cache hit rates: 123456789101112class ApplicationController &lt; ActionController::API after_action :log_cache_status private def log_cache_status  if response. status == 304   Rails. logger. info  [CACHE HIT] #{request. path}    # Increment your metrics here  end endendThe Gotchas: Gotcha 1: Middleware Order Matters: Rails middleware can modify responses after your controller runs. Make sure caching headers aren’t being overwritten: 12# config/application. rbconfig. middleware. insert_before Rack::ETag, YourFancyMiddlewareGotcha 2: Serializer Caching: If you’re using ActiveModel::Serializers or similar, ensure they respect caching: 1234567891011class ProductSerializer &lt; ActiveModel::Serializer cache key: 'product', expires_in: 1. hour attributes :id, :name, :price # This computed attribute could break caching attribute :current_discount do  # Make sure this is deterministic!  object. calculate_discount endendGotcha 3: Time Zones and Timestamps: Always use UTC for Last-Modified headers: 123if stale?(last_modified: @product. updated_at. utc) render json: @productendMeasuring Success: How do you know if your HTTP caching strategy is working? Look for:  Reduced average response times - 304 responses are typically 10x faster Lower database load - Fewer queries hitting your database Improved mobile app performance - Users see instant responses for cached data Reduced bandwidth costs - 304 responses have no bodyA well-cached API can handle 10x the traffic with the same infrastructure. Your Next Steps: HTTP caching isn’t a silver bullet, but it’s the closest thing we have in API performance. Start small:  Pick your most-requested endpoint Add simple Last-Modified caching Measure the impact Iterate from thereRemember: the fastest API request is the one that never hits your server. HTTP caching makes that possible without complex infrastructure or code changes. The best part? Your mobile developers will love you for it. Their apps will feel instantly responsive, work better offline, and consume less battery and data. That’s a win for everyone. References:  Rails Conditional GET Support MDN HTTP Caching Guide RFC 7234 - HTTP/1. 1 Caching"
    }, {
    "id": 10,
    "url": "https://prateekcodes.dev/rails-performance-80-20-rule/",
    "title": "Why your Rails performance fixes don't work (and how to find ones that do)",
    "body": "2025/06/28 - You’ve been there. The app feels sluggish. You start optimizing—replacing where. first with find_by, caching random method calls, switching from pluck to select. After hours of work, you deploy… and nothing changes. The app is still slow. Here’s why: you’re probably optimizing code that doesn’t matter. The uncomfortable truth about Rails performance: In most Rails applications, the vast majority of performance issues stem from a small fraction of your codebase. This isn’t speculation—it’s a pattern that emerges from the fundamental mathematics of how code executes in production, following what’s known as the Pareto Principle or 80/20 rule: roughly 80% of your performance problems come from just 20% of your code. Think about your typical Rails app:  Most requests hit a handful of popular endpoints Those endpoints usually call the same few service objects or models Within those, specific database queries or calculations dominate execution timeYet developers often spread optimization efforts evenly across the codebase, wasting time on code that barely impacts overall performance. Why guessing doesn’t work: Let me share a pattern I’ve seen repeatedly. A developer notices their app is slow and starts optimizing based on assumptions: 1234567891011#  This must be slow because it's in a loop! users. each do |user| # Spend hours optimizing this user. calculate_something_simpleend# Meanwhile, this innocent-looking line. . . @store = Store. includes(:products =&gt; [:variants, :images])       . where(featured: true)       . first# . . . is loading 10,000 product variants and 30,000 images into memoryThe loop might run 10 times with simple calculations. The “innocent” query might be loading your entire database into memory. Which one deserves optimization? Without measurement, you’re shooting in the dark. Finding the real bottlenecks: Here’s a systematic approach to identifying performance problems that actually matter: Step 1: Measure at the highest level: Start with your APM tool (New Relic, Scout, Skylight, etc. ) and look for patterns: 12Which endpoints consume the most total time?Total Time = Average Response Time × Request VolumeA 2-second endpoint hit once per day matters less than a 200ms endpoint hit 10,000 times per hour. Step 2: Profile the hot paths: Once you’ve identified problematic endpoints, profile them locally. Here’s a simple approach using rack-mini-profiler: 123456# Gemfilegroup :development do gem 'rack-mini-profiler' gem 'flamegraph' gem 'stackprof'endNow hit your endpoint with ?pp=flamegraph appended to see exactly where time goes. The flamegraph will reveal the truth through its visual stack trace: wide bars representing methods that consume the most time, with their child method calls stacked below. You might discover:  That innocent includes query spans 60% of the flamegraph width Multiple database round trips hidden in serializers Unexpected N+1 queries from lazy-loaded associations Heavy computation in methods you thought were trivialThe wider the bar, the more time that method consumes. Deep stacks reveal complex call chains that might be optimization opportunities. Step 3: Validate with production data: Local profiling uses development data. Before optimizing, validate against production patterns: 1234567891011121314151617181920# Simple production samplingclass ApplicationController &lt; ActionController::Base around_action :sample_performance  private  def sample_performance  return yield unless rand(100) == 1 # Sample 1% of requests    result = nil  time = Benchmark. realtime { result = yield }    if time &gt; 0. 5 # Log slow requests   Rails. logger. info  [SLOW] #{controller_name}##{action_name}: #{time}s    # Log additional context like user_id, params, etc.   end    result endendThe measurement-first workflow: Here’s the workflow that actually works:  Identify slow endpoints using production metrics Profile those specific endpoints to find bottlenecks Measure the impact of potential optimizations Implement only changes with meaningful impact Verify improvement in productionLet’s see this in action: 12345678910111213141516171819202122232425# You profile and find this query taking 800ms:def dashboard_data @projects = current_user. projects             . includes(:tasks, :members)             . where('created_at &gt; ?', 1. year. ago)end# Hypothesis: It's loading too much data# Measurement: How many records are we actually loading?Rails. logger. info  Loading #{@projects. count} projects Rails. logger. info  Total tasks: #{@projects. sum { |p| p. tasks. size }} # Result: 50 projects, 15,000 tasks!# Solution: Don't load everythingdef dashboard_data @projects = current_user. projects             . where('created_at &gt; ?', 1. year. ago)             . select(:id, :name, :status)  # Load counts separately @task_counts = Task. where(project_id: @projects. pluck(:id))           . group(:project_id)           . countend# Result: 50ms instead of 800msCommon bottleneck patterns in Rails: Through measurement, you’ll often find these patterns: 1. N+1 queries in serializers: 1234567# The profiler shows hundreds of identical queriesrender json: @posts, each_serializer: PostSerializer# Inside PostSerializerdef author_name object. author. name # N+1!end2. Loading unnecessary data: 12345# Profiler shows massive memory allocationUser. where(active: true) # Loading all columns for 10,000 users# When you only need:User. where(active: true). pluck(:id, :email)3. Missing database indexes: 1234567# Profiler shows long database timeOrder. where(user_id: params[:user_id], status: 'pending')# Check your query plan:Order. where(user_id: 1, status: 'pending'). explain# =&gt; Seq Scan on orders (cost=0. 00. . 1834. 00 rows=1 width=32)#  No index!Tools for measurement-driven optimization: Instead of guessing, use these tools: For production monitoring:  APM tools (New Relic, Scout, Skylight, Datadog) Custom logging and metrics Database slow query logsFor local profiling:  rack-mini-profiler - Real-time web UI ruby-prof - Detailed method-level profiling memory_profiler - Find memory bottlenecks benchmark-ips - Compare implementation optionsFor database analysis:  explain on ActiveRecord queries pg_stat_statements for PostgreSQL Query visualization toolsThe optimization decision framework: Before optimizing anything, ask:    Is this code in the critical path? If profiling shows it’s consuming &lt;5% of request time, move on.     What’s the potential impact? If you could make it 10x faster, would users notice?     What’s the implementation cost? A complex caching layer for a 10ms improvement rarely makes sense.     Can you measure the improvement? If you can’t measure it, you can’t improve it.  Conclusion: Stop optimizing code based on hunches. The vast majority of performance improvements come from fixing a tiny fraction of your codebase—but only if you identify the right fraction. Measure first. Profile second. Optimize third. This order matters. The next time your Rails app feels slow, resist the urge to start optimizing random code. Instead, check your metrics to identify problem areas, then profile those specific endpoints to find out where the time actually goes. You’ll be surprised how often the real bottleneck isn’t where you expected. "
    }, {
    "id": 11,
    "url": "https://prateekcodes.dev/rails-8-framework-defaults/",
    "title": "What are the new Rails 8 framework defaults?",
    "body": "2025/06/28 - When upgrading to Rails 8, running rails app:update generates a file called config/initializers/new_framework_defaults_8_0. rb. Rails 8’s framework defaults are relatively minimal compared to previous versions, focusing on timezone handling, HTTP caching behavior, and security improvements through regex timeout settings. ActiveSupport. to_time_preserves_timezone = :zone: 12345# Preserves the timezone when converting to `Time` with `to_time`. # `ActiveSupport. to_time_preserves_timezone = false` was the default behavior# before Rails 8. 0. With Rails 8. 0, the default becomes `:zone` which# preserves the timezone of the receiver when converting to `Time`. Rails. application. config. active_support. to_time_preserves_timezone = :zoneWhat this default does: This configuration changes how to_time methods handle timezone information. When set to :zone, the to_time method preserves the timezone of the receiver instead of converting to the system’s local timezone. Before Rails 8: 1234# Assuming system timezone is ESTtime_in_utc = Time. parse( 2024-01-01 12:00:00 UTC )time_in_utc. to_time# =&gt; 2024-01-01 07:00:00 -0500 (converted to system timezone)Rails 8 with :zone: 123time_in_utc = Time. parse( 2024-01-01 12:00:00 UTC )time_in_utc. to_time# =&gt; 2024-01-01 12:00:00 UTC (preserves original timezone)When to uncomment: Safe to uncomment if:  Your application expects to_time to preserve the original timezone You don’t have code that relies on automatic conversion to system timezone You want more predictable timezone behavior across different environmentsNot safe to uncomment if:  Your application relies on to_time converting to the local system timezone You have legacy code that expects the old behavior You process timestamps from external sources that need local timezone conversionSupporting changes needed: Due to a known issue, this configuration must be set in config/application. rb instead of the framework defaults initializer: 123456789# config/application. rbmodule YourApp class Application &lt; Rails::Application  config. load_defaults 8. 0    # This needs to be here, not in new_framework_defaults_8_0. rb  config. active_support. to_time_preserves_timezone = :zone endendActionDispatch. strict_freshness = true: 1234# Changes the behavior of `ActionController::ConditionalGet#fresh_when` and# `#stale?`, so that they honor the `If-None-Match` header before the# `If-Modified-Since` header, as specified by RFC 7232 section 6. Rails. application. config. action_dispatch. strict_freshness = trueWhat this default does: This setting aligns Rails with the HTTP specification (RFC 7232) for handling conditional GET requests. When enabled, ETag headers take precedence over Last-Modified headers when determining if a cached response is fresh. Before Rails 8: 123# Both ETag AND Last-Modified must match for 304 responsefresh_when(etag: @post, last_modified: @post. updated_at)# Client needs both headers to match for cache hitRails 8 with strict_freshness: 123# Only ETag needs to match for 304 response (per HTTP spec)fresh_when(etag: @post, last_modified: @post. updated_at)# If ETag matches, returns 304 regardless of Last-ModifiedWhen to uncomment: Safe to uncomment if:  You want RFC-compliant HTTP caching behavior Your caching strategy primarily relies on ETags You’re building a new API or have control over client implementationsNot safe to uncomment if:  Your application or clients depend on both ETag and Last-Modified matching You have custom caching logic that expects the historical Rails behavior Your CDN or proxy servers are configured for the old behaviorSupporting changes needed: Review your controller caching logic: 123456789# Check uses of fresh_when and stale?class PostsController &lt; ApplicationController def show  @post = Post. find(params[:id])    # This behavior changes with strict_freshness  fresh_when(@post) endendRegexp. timeout = 1: 1234567891011# Sets the default maximum amount of time a `Regexp` match can take, before a# `Regexp::TimeoutError` is raised. Defaults to `nil`, which means there is no# timeout. Can be configured with `Regexp. timeout=`. # # If set, this value applies to all Regex matching operations in the Ruby# process. This is a mitigation for potential Denial of Service attacks that# exploit certain Regex patterns. # # See https://stdgems. org/regexp_parser/Regexp. html#timeout-class_method for# more information. Regexp. timeout = 1What this default does: This sets a global 1-second timeout for all regular expression operations in your application, protecting against ReDoS (Regular Expression Denial of Service) attacks. 123456# Without timeout (potential DoS vulnerability) a  * 50 +  b  =~ /a+a+b/ # Can take exponential time# With 1-second timeoutRegexp. timeout = 1 a  * 50 +  b  =~ /a+a+b/ # Raises Regexp::TimeoutError after 1 secondWhen to uncomment: Safe to uncomment if:  You’re using Ruby 3. 2 or later (required for this feature) Your regexes are well-optimized and don’t require long execution times You want protection against ReDoS attacksNot safe to uncomment if:  You have complex regexes that legitimately take more than 1 second You’re processing large text files with regex operations You’re using an older Ruby version or alternative Ruby implementationSupporting changes needed: Identify and optimize slow regexes: 12345678910111213# Wrap potentially slow regex operationsbegin result = large_text. match(/complex_pattern/)rescue Regexp::TimeoutError # Handle timeout appropriately Rails. logger. warn  Regex timeout occurred  # Consider simplifying the regex or increasing timeoutend# Or set custom timeout for specific operationsRegexp. timeout = 5 do # Complex regex operation that needs more timeendConclusion: Rails 8’s framework defaults are focused and minimal, addressing specific concerns around timezone handling, HTTP specification compliance, and security. Unlike previous Rails versions, these changes are relatively safe to adopt, though the timezone configuration requires special attention due to the loading order issue. References:  PR #53490 - Default Regexp. timeout to 1s PR #52274 - Prefer ETag over Last-Modified for fresh_when and stale? Issue #54015 - Setting active_support. to_time_preserves_timezone in new_frameworks_default_8_0. rb does not work"
    }, {
    "id": 12,
    "url": "https://prateekcodes.dev/rails-read-replicas-part-3-production-excellence/",
    "title": "Scaling Rails with PostgreSQL Read Replicas: Part 3 - Production Excellence",
    "body": "2025/06/25 - In Part 1 and Part 2, we covered setup and advanced patterns. Now let’s focus on what happens when you actually deploy read replicas to production. This part covers the operational excellence required for production systems: how to deploy without downtime, monitor effectively, handle failures gracefully, and optimize performance based on real-world usage. These are the hard-won lessons from running read replicas at scale. What’s Covered:  Zero-Downtime Deployment Strategy Comprehensive Monitoring Performance Optimization Disaster Recovery Production ChecklistZero-Downtime Deployment Strategy: Adding read replicas to an existing production application requires careful planning. Here’s a battle-tested approach: Step 1: Start with 0% Traffic: Use the gradual rollout strategy from Part 2, starting with 0% traffic to replicas: 1234567891011# app/services/replica_rollout. rbclass ReplicaRollout ROLLOUT_PERCENTAGES = {  analytics_queries: 0,  # Start at 0%  search_queries: 0,    # Start at 0%  user_profiles: 0,    # Start at 0%  default: 0        # Everything uses primary }. freeze  # . . . rest of implementation from Part 2endDuring this phase:  Deploy replica infrastructure - Ensure replicas are receiving data Monitor replication lag - Verify replicas stay in sync Test with read-only users - Have internal team members manually test Collect baseline metrics - CPU, memory, query performance on replicas123456789101112131415161718192021222324252627282930313233343536373839404142434445# app/jobs/replica_health_validator_job. rbclass ReplicaHealthValidatorJob &lt; ApplicationJob def perform  # Check all replicas are healthy before increasing traffic  replicas = [:primary_replica, :primary_replica_2]    health_checks = replicas. map do |replica|   {    name: replica,    lag: check_replication_lag(replica),    connections: check_connection_count(replica),    query_success: test_query(replica)   }  end    if health_checks. all? { |check| check[:lag] &lt; 5. seconds &amp;&amp; check[:query_success] }   Rails. logger. info  All replicas healthy, ready for traffic    StatsD. event( replicas. ready_for_traffic ,  All health checks passed )  else   AlertService. notify( Replicas not ready , health_checks)  end end  private  def check_replication_lag(replica)  ApplicationRecord. connected_to(role: :reading, shard: replica) do   result = ApplicationRecord. connection. execute(     SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag    ). first   result['lag']. to_f. seconds  end rescue =&gt; e  Float::INFINITY end  def test_query(replica)  ApplicationRecord. connected_to(role: :reading, shard: replica) do   ApplicationRecord. connection. execute( SELECT 1 )   true  end rescue  false endendStep 2: Gradual Traffic Migration: Once shadow mode shows replicas are healthy, gradually migrate traffic: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# app/services/replica_traffic_manager. rbclass ReplicaTrafficManager MIGRATION_SCHEDULE = [  { percentage: 0, duration: 1. day },  # Shadow mode  { percentage: 5, duration: 1. hour },  # 5% of reads  { percentage: 10, duration: 2. hours }, # Monitor closely  { percentage: 25, duration: 4. hours },  { percentage: 50, duration: 1. day },  { percentage: 75, duration: 1. day },  { percentage: 100, duration: nil }   # Full migration ]. freeze  def self. current_percentage  migration_start = Rails. cache. read('replica_migration_start') || Time. current  elapsed = Time. current - migration_start    MIGRATION_SCHEDULE. each_with_index do |phase, index|   next_phase = MIGRATION_SCHEDULE[index + 1]      if phase[:duration]. nil? || elapsed &lt; phase[:duration]    return phase[:percentage]   end      elapsed -= phase[:duration]  end    100 # Full migration complete end  def self. should_use_replica?(feature: :default)  percentage = current_percentage    # Critical features migrate last  percentage = [percentage - 20, 0]. max if [:payments, :auth]. include?(feature)    rand(100) &lt; percentage end  def self. with_traffic_management(feature: :default, &amp;block)  if should_use_replica?(feature: feature) &amp;&amp; ReplicaHealthCheck. healthy?   ApplicationRecord. connected_to(role: :reading, &amp;block)  else   yield  end rescue =&gt; e  # Always fallback to primary on errors  ErrorTracker. track(e, context: { feature: feature })  yield endendStep 3: Automated Rollback: Build automatic rollback when issues arise: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# app/services/replica_circuit_breaker. rbclass ReplicaCircuitBreaker FAILURE_THRESHOLD = 5 TIMEOUT_THRESHOLD = 10. seconds RECOVERY_TIMEOUT = 1. minute  class &lt;&lt; self  def call(&amp;block)   return yield if circuit_open?      execute_with_breaker(&amp;block)  end    private    def execute_with_breaker(&amp;block)   start_time = Time. current      result = ApplicationRecord. connected_to(role: :reading, &amp;block)      # Reset failures on success   reset_failure_count   result     rescue =&gt; e   record_failure(e, Time. current - start_time)      # Fallback to primary   yield  end    def circuit_open?   return false unless last_failure_time      # Check if we're still in recovery period   Time. current - last_failure_time &lt; RECOVERY_TIMEOUT  end    def record_failure(error, duration)   increment_failure_count      if failure_count &gt;= FAILURE_THRESHOLD || duration &gt; TIMEOUT_THRESHOLD    open_circuit    AlertService. critical(      Replica circuit breaker opened ,     error: error. message,     failure_count: failure_count,     duration: duration    )   end  end    def open_circuit   Rails. cache. write('replica:circuit:open', true, expires_in: RECOVERY_TIMEOUT)   Rails. cache. write('replica:circuit:opened_at', Time. current)  end    def failure_count   Rails. cache. read('replica:circuit:failures'). to_i  end    def increment_failure_count   Rails. cache. increment('replica:circuit:failures', 1, expires_in: 5. minutes)  end    def reset_failure_count   Rails. cache. delete('replica:circuit:failures')  end    def last_failure_time   Rails. cache. read('replica:circuit:opened_at')  end endendComprehensive Monitoring: Production read replicas need monitoring at multiple levels. Most teams already use APM tools like New Relic, DataDog, or AppSignal—leverage these instead of building custom monitoring. APM Integration: Modern APM tools automatically track database metrics, but you need to ensure they distinguish between primary and replica queries: 1234567891011121314151617# config/initializers/datadog. rb (if using DataDog)Datadog. configure do |c| c. tracing. instrument :active_record, service_name: 'postgres' do |config|  # Tag queries by database role  config. on_query do |span, event|   connection = event. payload[:connection]   role = connection. pool. db_config. configuration_hash[:replica] ? 'replica' : 'primary'      span. set_tag('db. role', role)   span. set_tag('db. connection_name', connection. pool. db_config. name)  end endend# For New Relic# config/newrelic. yml# Enable database query analysis to see replica vs primary distributionCloud Provider Monitoring: If using managed databases, leverage their built-in monitoring: AWS RDS: 123456789# CloudWatch already tracks these for RDS read replicas:# - ReplicaLag# - ReadIOPS / WriteIOPS# - DatabaseConnections# - CPUUtilization per replica# Just add CloudWatch alarms:# Alarm: ReplicaLag &gt; 5000 milliseconds# Alarm: DatabaseConnections &gt; 80% of max_connectionsGoogle Cloud SQL / Azure Database:Similar built-in metrics available through their monitoring services. Custom Metrics for Business Logic: While APM tools handle infrastructure metrics, you still need application-specific monitoring: 12345678910111213141516171819202122232425262728293031# app/controllers/application_controller. rbclass ApplicationController &lt; ActionController::Base around_action :track_replica_usage  private  def track_replica_usage  replica_used = false    # Hook into ActiveRecord to detect replica usage  subscriber = ActiveSupport::Notifications. subscribe('sql. active_record') do |*args|   event = ActiveSupport::Notifications::Event. new(*args)   connection = event. payload[:connection]      if connection&amp;. pool&amp;. db_config&amp;. configuration_hash&amp;. dig(:replica)    replica_used = true   end  end    yield    # Send to your APM  if defined?(Datadog)   Datadog::Tracing. active_trace&amp;. set_tag('replica. used', replica_used)  elsif defined?(NewRelic)   NewRelic::Agent. add_custom_attributes(replica_used: replica_used)  end ensure  ActiveSupport::Notifications. unsubscribe(subscriber) endendKey Metrics to Monitor: Configure your APM dashboard to track:  Infrastructure (from CloudWatch/APM):     Replication lag per replica   Connection count by database role   Query response time P50/P95/P99 by role   Error rate by database    Application (from APM custom metrics):     % of requests using replicas   Cache hit rate (higher replica usage should increase this)   Replica fallback rate (when replicas fail)    Business Impact:     Page load time before/after replica adoption   API response times by endpoint   Background job duration changes   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# app/services/replica_monitor. rbclass ReplicaMonitor def self. check_all  {   replication_lag: check_replication_lag,   connection_stats: check_connections,   query_performance: check_query_performance,   disk_usage: check_disk_usage,   long_running_queries: check_long_queries  } end  private  def self. check_replication_lag  replicas = ApplicationRecord. configurations. configs_for(role: :reading)    replicas. map do |config|   ApplicationRecord. connected_to(role: :reading, shard: config. name. to_sym) do    lag_query = &lt;&lt;-SQL     SELECT       pg_is_in_recovery() as is_replica,      pg_last_wal_receive_lsn() as receive_lsn,      pg_last_wal_replay_lsn() as replay_lsn,      EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag_seconds,      pg_last_xact_replay_timestamp() as last_replay_time    SQL        result = ApplicationRecord. connection. exec_query(lag_query). first        {     name: config. name,     lag_seconds: result['lag_seconds']&amp;. to_f || 0,     lag_bytes: calculate_lag_bytes(result['receive_lsn'], result['replay_lsn']),     last_replay: result['last_replay_time'],     healthy: (result['lag_seconds']&amp;. to_f || 0) &lt; 5. 0    }   end  end end  def self. check_connections  ApplicationRecord. connected_to(role: :reading) do   stats = ApplicationRecord. connection. exec_query(&lt;&lt;-SQL). first    SELECT      COUNT(*) FILTER (WHERE state = 'active') as active_connections,     COUNT(*) FILTER (WHERE state = 'idle') as idle_connections,     COUNT(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction,     COUNT(*) as total_connections,     MAX(EXTRACT(EPOCH FROM (now() - state_change))) as longest_connection_seconds    FROM pg_stat_activity    WHERE pid &lt;&gt; pg_backend_pid()   SQL      {    active: stats['active_connections'],    idle: stats['idle_connections'],    idle_in_transaction: stats['idle_in_transaction'],    total: stats['total_connections'],    longest_duration: stats['longest_connection_seconds']&amp;. to_f,    pool_exhaustion_risk: stats['total_connections'] &gt; 80   }  end end  def self. check_query_performance  ApplicationRecord. connected_to(role: :reading) do   slow_queries = ApplicationRecord. connection. exec_query(&lt;&lt;-SQL)    SELECT      query,     calls,     total_time,     mean_time,     max_time,     stddev_time    FROM pg_stat_statements    WHERE query NOT LIKE '%pg_stat_statements%'    ORDER BY mean_time DESC    LIMIT 10   SQL      slow_queries. map(&amp;:to_h)  end end  def self. calculate_lag_bytes(receive_lsn, replay_lsn)  return 0 unless receive_lsn &amp;&amp; replay_lsn    # PostgreSQL LSN format: XXXXXXXX/YYYYYYYY  receive_bytes = lsn_to_bytes(receive_lsn)  replay_bytes = lsn_to_bytes(replay_lsn)    receive_bytes - replay_bytes end  def self. lsn_to_bytes(lsn)  high, low = lsn. split('/'). map { |x| x. to_i(16) }  (high &lt;&lt; 32) + low endendApplication-Level Monitoring: Track how your application uses replicas: 1234567891011121314151617181920212223242526272829303132333435363738# config/initializers/replica_instrumentation. rbActiveSupport::Notifications. subscribe('sql. active_record') do |*args| event = ActiveSupport::Notifications::Event. new(*args)  # Determine which connection was used connection_name = event. payload[:connection]&amp;. pool&amp;. db_config&amp;. name || 'unknown' role = event. payload[:connection]&amp;. pool&amp;. db_config&amp;. configuration_hash&amp;. dig(:replica) ? 'replica' : 'primary'  # Track metrics tags = {  role: role,  connection: connection_name,  operation: extract_operation(event. payload[:sql]) }  StatsD. histogram('db. query. duration', event. duration, tags: tags) StatsD. increment('db. query. count', tags: tags)  # Alert on slow replica queries if role == 'replica' &amp;&amp; event. duration &gt; 1000  Rails. logger. warn(    Slow replica query ,   duration: event. duration,   sql: event. payload[:sql],   connection: connection_name  ) endenddef extract_operation(sql) case sql when /^SELECT/i then 'select' when /^INSERT/i then 'insert' when /^UPDATE/i then 'update' when /^DELETE/i then 'delete' else 'other' endendPerformance Optimization: 1. Query Optimization for Replicas: Some queries perform differently on replicas: 1234567891011121314151617181920212223242526272829303132333435363738394041424344# app/models/concerns/replica_optimized. rbmodule ReplicaOptimized extend ActiveSupport::Concern  class_methods do  def replica_optimized_scope(name, &amp;block)   # Define two versions of the scope   scope  #{name}_primary , block   scope  #{name}_replica , block      # Smart scope that picks the right version   define_singleton_method(name) do |*args|    if ApplicationRecord. current_role == :reading     send( #{name}_replica , *args)    else     send( #{name}_primary , *args)    end   end  end endendclass Product &lt; ApplicationRecord include ReplicaOptimized  # Regular scope for primary replica_optimized_scope :search do |term|  where( name ILIKE ? ,  %#{term}% ) end  # Override for replica with better performance def self. search_replica(term)  # Use full-text search on replica (assuming GIN index)  where( search_vector @@ plainto_tsquery('english', ?) , term) end  # Materialized view only on replica def self. top_selling_replica  # This materialized view is refreshed hourly on replicas  connection. exec_query(    SELECT * FROM top_selling_products_mv LIMIT 100   ) endend2. Connection Pool Tuning: Optimize pools based on actual usage: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# app/services/connection_pool_optimizer. rbclass ConnectionPoolOptimizer def self. tune_pools  configs = ApplicationRecord. configurations. configs_for(env_name: Rails. env)    configs. each do |config|   pool = ApplicationRecord. connection_handler. retrieve_connection_pool(config. name)   next unless pool      stats = pool_statistics(pool)      if stats[:wait_count] &gt; 10    # Pool is too small    recommendation = (pool. size * 1. 5). to_i    Rails. logger. info(      Pool #{config. name} should be increased to #{recommendation} ,     current_size: pool. size,     wait_count: stats[:wait_count]    )   elsif stats[:usage_ratio] &lt; 0. 3    # Pool is too large    recommendation = [(pool. size * 0. 7). to_i, 5]. max    Rails. logger. info(      Pool #{config. name} could be reduced to #{recommendation} ,     current_size: pool. size,     usage_ratio: stats[:usage_ratio]    )   end  end end  def self. pool_statistics(pool)  {   size: pool. size,   connections: pool. connections. size,   busy: pool. connections. count(&amp;:in_use?),   dead: pool. connections. count(&amp;:disconnected?),   wait_count: pool. stat[:wait_count],   usage_ratio: pool. connections. count(&amp;:in_use?). to_f / pool. size  } endend# app/jobs/connection_pool_monitor_job. rbclass ConnectionPoolMonitorJob &lt; ApplicationJob queue_as :monitoring  def perform  ConnectionPoolOptimizer. tune_pools    # Schedule next check  self. class. set(wait: 5. minutes). perform_later endend# Start monitoring (in an initializer or deploy task)ConnectionPoolMonitorJob. perform_later if Rails. env. production?3. Multi-Region Optimization: For global applications, use region-aware routing: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# app/services/region_aware_router. rbclass RegionAwareRouter REGION_REPLICAS = {  # Map your regions to database configurations  # 'region_key' =&gt; :database_config_name  # Example:  # 'us-east' =&gt; :primary_replica_us_east,  # 'eu' =&gt; :primary_replica_eu, }. freeze  def self. nearest_replica(request)  region = detect_region(request)  REGION_REPLICAS[region] || :primary_replica # Fallback to default replica end  def self. with_nearest_replica(request, &amp;block)  replica = nearest_replica(request)    ApplicationRecord. connected_to(role: :reading, shard: replica) do   yield  end rescue =&gt; e  # Critical: Always fallback to a working replica  Rails. logger. warn( Region replica failed: #{replica}, error: #{e. message} )  StatsD. increment('replica. region_fallback', tags: [ region:#{replica} ])    # Fallback strategy - could be primary or another region  ApplicationRecord. connected_to(role: :reading, &amp;block) end  private  def self. detect_region(request)  # Implement based on your infrastructure:    # Option 1: CDN/Proxy headers  # request. headers['YOUR-CDN-REGION-HEADER']    # Option 2: Load balancer headers  # request. headers['X-AWS-REGION'] or custom headers    # Option 3: GeoIP lookup (implement your preferred service)  # GeoIP. lookup(request. remote_ip). region_code    # Option 4: User preference/account setting  # current_user&amp;. preferred_region    # Implement your detection logic here  # Return a key that matches REGION_REPLICAS endend# Usage pattern - apply selectivelyclass ApplicationController &lt; ActionController::Base # Only use for read-heavy, latency-sensitive endpoints def with_regional_replica(&amp;block)  if should_use_regional_routing?   RegionAwareRouter. with_nearest_replica(request, &amp;block)  else   yield # Use default routing  end end  private  def should_use_regional_routing?  # Implement your logic:  # - Feature flag controlled  # - Only for certain controllers/actions  # - Only for premium users  # - etc.  endend# Example usage in specific controllersclass ApiController &lt; ApplicationController def index  with_regional_replica do   @data = YourModel. complex_read_query  end endendDisaster Recovery: Automatic Failover: Handle replica failures gracefully: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# app/services/replica_failover_manager. rbclass ReplicaFailoverManager class &lt;&lt; self  def healthy_replicas   @healthy_replicas ||= {}  end    def mark_unhealthy(replica_name, error)   healthy_replicas[replica_name] = {    healthy: false,    error: error. message,    failed_at: Time. current   }      AlertService. notify(     Replica marked unhealthy: #{replica_name} ,    error: error. message   )  end    def check_replica_health(replica_name)   return false if healthy_replicas[replica_name]&amp;. dig(:healthy) == false      ApplicationRecord. connected_to(role: :reading, shard: replica_name) do    ApplicationRecord. connection. exec_query( SELECT 1 )    true   end  rescue =&gt; e   mark_unhealthy(replica_name, e)   false  end    def with_failover(&amp;block)   available_replicas = [:primary_replica, :primary_replica_2, :primary_replica_3]      available_replicas. each do |replica|    next unless check_replica_health(replica)        begin     return ApplicationRecord. connected_to(role: :reading, shard: replica, &amp;block)    rescue =&gt; e     Rails. logger. error( Replica #{replica} failed during query: #{e. message} )     mark_unhealthy(replica, e)    end   end      # All replicas failed, use primary   Rails. logger. warn( All replicas unhealthy, falling back to primary )   yield  end end  # Background health checker job class HealthCheckerJob &lt; ApplicationJob  queue_as :monitoring    def perform   ReplicaFailoverManager. healthy_replicas. each do |replica_name, status|    next if status[:healthy]        # Try to recover unhealthy replicas    if Time. current - status[:failed_at] &gt; 5. minutes     if ReplicaFailoverManager. check_replica_health(replica_name)      ReplicaFailoverManager. healthy_replicas[replica_name] = { healthy: true }      AlertService. notify( Replica recovered: #{replica_name} )     end    end   end      # Schedule next check   self. class. set(wait: 30. seconds). perform_later  end endendProduction Checklist: Before going live with read replicas:  Monitoring     Replication lag alerts (&lt; 5 seconds)   Connection pool monitoring   Query performance tracking   Error rate monitoring    Testing     Load testing with realistic read/write ratios   Failover testing   Replication lag simulation   Connection pool exhaustion testing    Operations     Runbook for replica issues   Automated health checks   Circuit breakers configured   Gradual rollout plan    Performance     Replica-specific indexes created   Connection pools tuned   Query routing optimized   Region-aware routing (if needed)   Further Reading: For production deployment best practices:  PostgreSQL Documentation     Monitoring Database Activity - Understanding pg_stat views   High Availability, Load Balancing, and Replication - Production deployment patterns   Connection Pooling - Tuning connection parameters    AWS/Cloud Resources     Amazon RDS Read Replicas - If using RDS   Working with PostgreSQL Read Replicas - RDS PostgreSQL specific guide   Key Takeaways: Successfully running read replicas in production requires:  Gradual adoption - Start with shadow mode, increase traffic slowly Comprehensive monitoring - Track everything from replication lag to query patterns Automatic recovery - Build systems that heal themselves Performance focus - Optimize specifically for replica characteristicsRead replicas are powerful but complex. Start simple, measure everything, and build sophistication as your needs grow. "
    }, {
    "id": 13,
    "url": "https://prateekcodes.dev/rails-read-replicas-part-2-advanced-patterns/",
    "title": "Scaling Rails with PostgreSQL Read Replicas: Part 2 - Advanced Patterns and Gotchas",
    "body": "2025/06/25 - In Part 1, we covered the basics of setting up read replicas. Now let’s tackle the challenging aspects that make the difference between a system that works in development and one that thrives in production. What’s Covered:  The Replication Lag Challenge Implementing Sticky Sessions Advanced Query Routing Patterns Connection Pool Management Handling Edge Cases Testing with ReplicasThe Replication Lag Challenge: The biggest challenge with read replicas is replication lag—the delay between when data is written to the primary and when it appears on replicas. Let’s understand why this matters. The “Invisible Update” Problem: Here’s a scenario that can be frustrating: 12345678910111213class ProfilesController &lt; ApplicationController def update  @user = current_user  @user. update!(bio: params[:bio])    redirect_to profile_path(@user) end  def show  # If this runs on a replica, user might see old data!  @user = User. find(params[:id]) endendThe user updates their bio and gets redirected, but they see their old bio because the show action read from a replica that hadn’t received the update yet. Implementing Sticky Sessions: Rails provides automatic connection switching to solve this problem through the DatabaseSelector middleware. Basic Configuration: 12345678# config/environments/production. rbRails. application. configure do config. active_record. database_selector = { delay: 2. seconds } config. active_record. database_resolver =   ActiveRecord::Middleware::DatabaseSelector::Resolver config. active_record. database_resolver_context =   ActiveRecord::Middleware::DatabaseSelector::Resolver::SessionendThis configuration creates “sticky sessions”—after a write operation, that user’s session reads from the primary database for 2 seconds, ensuring they see their own changes. You can generate this configuration automatically using: 1rails generate active_record:multi_db(See Rails Guide on Activating Automatic Role Switching) How Sticky Sessions Work: Let’s trace through what happens: 1234567891011121314151617181920212223# 1. User makes a POST request to update their profile# POST /profiledef update current_user. update!(bio:  New bio ) # Write to primary # Rails automatically stores timestamp: session[:last_write_timestamp] = Time. current redirect_to profile_pathend# 2. Browser follows redirect# GET /profiledef show # Rails middleware checks: Time. current - session[:last_write_timestamp] &lt; 2. seconds # Since it's within 2 seconds, this query goes to PRIMARY, not replica @user = current_userend# 3. User refreshes page 3 seconds later# GET /profiledef show # Now: Time. current - session[:last_write_timestamp] &gt; 2. seconds # This query can safely go to REPLICA @user = current_userendCustomizing Sticky Session Behavior: The default 2-second delay works for many apps, but sometimes you need more control. For example:  Financial transactions need longer consistency windows Critical paths like checkout flows should always use primary Different operations have different consistency requirementsHere’s a custom resolver that addresses these needs: 12345678910111213141516171819202122232425262728293031323334353637383940414243# app/middleware/custom_database_resolver. rbclass CustomDatabaseResolver &lt; ActiveRecord::Middleware::DatabaseSelector::Resolver CRITICAL_PATHS = %w[/checkout /payment /account]. freeze  def read_from_primary?(request)  # Always use primary for critical paths  return true if CRITICAL_PATHS. any? { |path| request. path. start_with?(path) }    # Check if we recently wrote data  return true if recently_wrote?(request)    # Use primary for non-GET requests  !request. get? &amp;&amp; !request. head? end  private  def recently_wrote?(request)  last_write = last_write_timestamp(request)  return false unless last_write    # Different delays for different operations  delay = if request. session[:critical_write]   10. seconds # Financial operations need longer consistency  else   2. seconds  # Normal operations  end    Time. current - last_write &lt; delay end  def last_write_timestamp(request)  timestamp = request. session[:last_write_timestamp]  return nil unless timestamp    Time. at(timestamp. to_i) endend# Use the custom resolverRails. application. configure do config. active_record. database_resolver = CustomDatabaseResolverendAdvanced Query Routing Patterns: Beyond sticky sessions, you need patterns for routing specific queries intelligently. Pattern 1: Smart Query Router: Build a router that intelligently decides where queries should go: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# app/services/smart_query_router. rbclass SmartQueryRouter # Queries safe for replicas even with lag REPLICA_SAFE_PATTERNS = {  analytics: /GROUP BY|COUNT\(\*\)|SUM\(|AVG\(/i,  historical: /created_at &lt; '#{1. hour. ago}'/,  reference_data: /countries|currencies|categories/ }. freeze  def self. route(model_class, &amp;block)  sql = capture_sql(&amp;block)    if should_use_replica?(model_class, sql)   model_class. connected_to(role: :reading, &amp;block)  else   yield  end end  private  def self. should_use_replica?(model_class, sql)  # Never use replica for write operations  return false if sql =~ /INSERT|UPDATE|DELETE/i    # Check if query matches safe patterns  REPLICA_SAFE_PATTERNS. any? do |_type, pattern|   sql =~ pattern  end end  def self. capture_sql(&amp;block)  queries = []  subscriber = ActiveSupport::Notifications. subscribe('sql. active_record') do |*args|   event = ActiveSupport::Notifications::Event. new(*args)   queries &lt;&lt; event. payload[:sql]  end    yield  queries. join(' ') ensure  ActiveSupport::Notifications. unsubscribe(subscriber) endend# UsageSmartQueryRouter. route(Order) do Order. where('created_at &lt; ?', 1. month. ago). sum(:total)end# Automatically routed to replica because it's historical dataPattern 2: Gradual Replica Adoption: Rolling out replicas gradually reduces risk: 123456789101112131415161718192021222324252627282930313233343536373839404142# app/services/replica_rollout. rbclass ReplicaRollout ROLLOUT_PERCENTAGES = {  analytics_queries: 100, # 100% of analytics use replicas  search_queries: 50,   # 50% of searches use replicas  user_profiles: 10,    # 10% of profile reads use replicas  default: 0        # Everything else uses primary }. freeze  def self. with_smart_routing(query_type = :default, &amp;block)  percentage = ROLLOUT_PERCENTAGES[query_type] || 0    if should_use_replica?(percentage)   begin    ApplicationRecord. connected_to(role: :reading, &amp;block)   rescue =&gt; e    # Fallback to primary on any replica issues    Rails. logger. error  Replica failed: #{e. message}, falling back to primary     yield   end  else   yield  end end  private  def self. should_use_replica?(percentage)  # Use request ID for consistent routing per request  request_id = Thread. current[:request_id] || SecureRandom. uuid  Digest::MD5. hexdigest(request_id). to_i(16) % 100 &lt; percentage endend# Usage in controllersclass SearchController &lt; ApplicationController def index  ReplicaRollout. with_smart_routing(:search_queries) do   @results = Product. search(params[:q])  end endendPattern 3: Read-Your-Writes for Specific Models: The need for immediate consistency could be at the model level. Here’s a way to achieve that: 1234567891011121314151617181920212223242526# app/models/concerns/immediate_consistency. rbmodule ImmediateConsistency extend ActiveSupport::Concern  included do  # Override connection to always use primary for this model  def self. connection   return super unless ActiveRecord::Base. current_role == :reading      ActiveRecord::Base. connected_to(role: :writing) do    return super   end  end endend# Models that need immediate consistencyclass PaymentTransaction &lt; ApplicationRecord include ImmediateConsistency # All queries for this model use primary, even in a reading blockendclass UserSession &lt; ApplicationRecord include ImmediateConsistency # Session data must always be currentend⚠️ Warning: I do not recommend this pattern unless you know what you’re doing. It can be very easy to go wrong because:  The behavior is “magical” and not obvious to other developers Users of this model may see unintended behavior It overrides core Rails methods in non-obvious ways Debugging becomes difficult when queries don’t go where expectedConsider explicit methods or service objects instead for better clarity. Connection Pool Management: Read replicas require careful connection pool management to avoid exhaustion: Understanding the Problem: When you add read replicas, your connection usage multiplies. Each database configuration maintains its own connection pool, and these pools exist per process. Consider a typical setup:  1 primary database 2 read replicas 5 connections per pool (Rails default) 10 Puma workersThis creates 150 total database connections (3 databases × 5 connections × 10 workers). Many databases have connection limits—PostgreSQL defaults to 100 connections. You’ll hit the limit before your application even starts serving traffic. For a deeper understanding of connection pools, see the Rails Connection Pool documentation and this excellent article on PostgreSQL connection pooling. Optimizing Connection Pools: The key is to right-size your pools based on actual usage patterns. Primary databases handle all writes and need more connections, while replicas only handle reads and can work with fewer connections. Additionally, replicas can return idle connections more aggressively since read queries are typically shorter. Here’s how to configure pools efficiently: 12345678910111213141516171819202122232425262728293031323334353637# config/database. ymlproduction: primary:  pool: &lt;%= ENV. fetch( PRIMARY_DB_POOL , 5) %&gt;  # Keep more connections for primary (handles all writes)   primary_replica:  pool: &lt;%= ENV. fetch( REPLICA_DB_POOL , 3) %&gt;  # Fewer connections per replica, but we have multiple replicas  idle_timeout: 300 # Return idle connections faster  checkout_timeout: 2 # Fail fast if no connections available   primary_replica_2:  pool: &lt;%= ENV. fetch( REPLICA_DB_POOL , 3) %&gt;  idle_timeout: 300  checkout_timeout: 2# app/models/application_record. rbclass ApplicationRecord &lt; ActiveRecord::Base self. abstract_class = true  # Distribute reads across multiple replicas connects_to database: {   writing: :primary,  reading: [:primary_replica, :primary_replica_2]. sample }  # Better: Use a load balancer class &lt;&lt; self  def reading_connection   replicas = [:primary_replica, :primary_replica_2]   replica = LoadBalancer. least_connections(replicas)      configurations. configs_for(env_name: Rails. env, name: replica. to_s)  end endendPer-Feature Connection Pools: Different features need different pool sizes: 123456789101112131415161718192021# config/database. ymlproduction: # Analytics jobs need more connections analytics_replica:  &lt;&lt;: *replica_config  pool: 10   # API endpoints need fewer but faster connections  api_replica:  &lt;&lt;: *replica_config  pool: 3  checkout_timeout: 1  # app/jobs/analytics_job. rbclass AnalyticsJob &lt; ApplicationJob around_perform do |job, block|  ApplicationRecord. connected_to(role: :reading, shard: :analytics_replica) do   block. call  end endendHandling Edge Cases: 1. Cross-Database Joins: Read replicas complicate joins across different models: 12345678910111213141516171819# This breaks with replicasdef user_with_recent_orders ApplicationRecord. connected_to(role: :reading) do  user = User. find(params[:id]) # From replica end  # This might use primary, causing a cross-database join attempt user. orders. where('created_at &gt; ?', 1. day. ago)end# Solution: Load everything in the same connection blockdef user_with_recent_orders ApplicationRecord. connected_to(role: :reading) do  User. includes(:orders)    . where(id: params[:id])    . where(orders: { created_at: 1. day. ago. . })    . first endend2. Transactions Across Connections: 12345678910111213141516171819# This won't work as expectedApplicationRecord. transaction do user = User. create!(name:  Alice ) # Primary  ApplicationRecord. connected_to(role: :reading) do  # This runs outside the transaction!  Analytics. create!(user_id: user. id) endend# Solution: Keep transactions on single connectionApplicationRecord. transaction do user = User. create!(name:  Alice )  # Explicitly use primary for analytics within transaction Analytics. connected_to(role: :writing) do  Analytics. create!(user_id: user. id) endendTesting with Replicas: Testing replica behavior requires special setup: 1234567891011121314151617181920212223242526272829303132333435363738394041424344# spec/support/replica_test_helper. rbmodule ReplicaTestHelper def with_replica_lag(seconds)  # Simulate lag by delaying replica queries  allow(ApplicationRecord). to receive(:connected_to). and_wrap_original do |method, **options, &amp;block|   if options[:role] == :reading    sleep(seconds)   end   method. call(**options, &amp;block)  end    yield end  def assert_uses_replica(&amp;block)  expect(ApplicationRecord). to receive(:connected_to). with(role: :reading)  block. call end  def assert_uses_primary(&amp;block)  expect(ApplicationRecord). not_to receive(:connected_to). with(role: :reading)  block. call endend# spec/controllers/profiles_controller_spec. rbRSpec. describe ProfilesController do include ReplicaTestHelper  it  uses primary database after writes  do  post :update, params: { bio:  New bio  }    assert_uses_primary do   get :show  end end  it  handles replica lag gracefully  do  with_replica_lag(0. 5) do   get :analytics   expect(response). to be_successful  end endendWhat’s Next?: Make sure to check out:  Part 3 - Production ExcellenceRemember: complexity should match your needs. Start with Rails’ built-in connection switching and add custom routing as specific use cases demand it. "
    }, {
    "id": 14,
    "url": "https://prateekcodes.dev/rails-read-replicas-part-1-understanding-the-basics/",
    "title": "Scaling Rails with PostgreSQL Read Replicas: Part 1 - Understanding the Basics",
    "body": "2025/06/25 - In this three-part series, we’ll explore how to effectively use PostgreSQL read replicas with Rails applications. While the official Rails guide covers the configuration basics, implementing read replicas in production requires understanding the nuances that can make or break your application’s performance. When Do You Actually Need Read Replicas?: Before diving into implementation, let’s understand if read replicas are the right solution for your scaling challenges. Signs You Need Read Replicas: Your application might benefit from read replicas when you see these key indicators:  Read-heavy workload: Your read:write ratio exceeds 80:20 Long-running queries: Reports and analytics slow down transactional queries Different access patterns: OLTP (fast transactions) vs OLAP (complex analytics) Geographic distribution: Users in different regions need low-latency readsDoes this query seem familiar? 123456# Your analytics queries are blocking user-facing featuresUser. joins(:orders)  . group('users. id')  . having('COUNT(orders. id) &gt; ?', 100)  . pluck('users. email, COUNT(orders. id), SUM(orders. total)')# This query takes 5+ seconds and runs frequentlyWhen Read Replicas Won’t Help: Read replicas are not a silver bullet. They won’t help if:  Write bottlenecks: Your database is slow due to heavy writes Poor query performance: Unoptimized queries will be slow on replicas too Real-time requirements: If you can’t tolerate any replication lagUnderstanding PostgreSQL Streaming Replication: PostgreSQL uses streaming replication to keep replicas synchronized. Here’s what happens under the hood: 123456-- On the primary databaseINSERT INTO orders (user_id, total) VALUES (1, 99. 99);-- This generates a WAL (Write-Ahead Log) record-- The WAL record streams to replicas-- Replicas apply the change, but with a slight delayThis delay, called replication lag, is crucial to understand: 123456789# On primaryuser = User. create!(name:  Jane )# user. id = 123# Immediately on replica (within milliseconds)User. find(123) # =&gt; ActiveRecord::RecordNotFound# After replication lag (typically &lt; 1 second)User. find(123) # =&gt; #&lt;User id: 123, name:  Jane &gt;Setting Up Your First Read Replica: Let’s implement a basic read replica setup that handles the most common use case: offloading analytics queries. Step 1: Database Configuration: First, configure your database. yml to define both primary and replica connections: 1234567891011121314151617production: primary:  adapter: postgresql  host: &lt;%= ENV['PRIMARY_DB_HOST'] %&gt;  database: myapp_production  username: &lt;%= ENV['DB_USERNAME'] %&gt;  password: &lt;%= ENV['DB_PASSWORD'] %&gt;  pool: &lt;%= ENV. fetch( RAILS_MAX_THREADS ) { 5 } %&gt;   primary_replica:  adapter: postgresql  host: &lt;%= ENV['REPLICA_DB_HOST'] %&gt;  database: myapp_production  username: &lt;%= ENV['DB_REPLICA_USERNAME'] %&gt;  password: &lt;%= ENV['DB_REPLICA_PASSWORD'] %&gt;  pool: &lt;%= ENV. fetch( RAILS_MAX_THREADS ) { 5 } %&gt;  replica: true # This marks it as a read-only connectionThe replica: true flag is important—it tells Rails this connection should never receive writes. Step 2: Model Configuration: Now, tell your models about these connections: 12345678class ApplicationRecord &lt; ActiveRecord::Base self. abstract_class = true  connects_to database: {   writing: :primary,   reading: :primary_replica  }endThis configuration means:  All write operations (INSERT, UPDATE, DELETE) go to :primary Read operations can be directed to :primary_replica By default, all operations still go to primary (for safety)Step 3: Basic Read/Write Splitting: Here’s how to explicitly route queries to your replica: 1234567891011121314151617181920class AnalyticsController &lt; ApplicationController def revenue_report  # This block ensures all queries inside use the replica  ApplicationRecord. connected_to(role: :reading) do   @revenue_by_month = Order    . group( DATE_TRUNC('month', created_at) )    . sum(:total)       @top_customers = User    . joins(:orders)    . group('users. id')    . order('SUM(orders. total) DESC')    . limit(10)    . pluck('users. name, SUM(orders. total)')  end    # Queries here go back to primary  current_user. update!(last_viewed_report_at: Time. current) endendThe connected_to block is like a database transaction—everything inside it uses the specified connection. Real-World Example: Separating Analytics: Let’s build a practical example where analytics queries never impact your main application: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# app/models/analytics/base. rbmodule Analytics class Base &lt; ApplicationRecord  self. abstract_class = true    # Always use replica for analytics models  connects_to database: {    writing: :primary,   reading: :primary_replica   }    # Force all queries to use replica  def self. default_role   :reading  end endend# app/models/analytics/user_metrics. rbmodule Analytics class UserMetrics &lt; Base  self. table_name = 'users'    def self. monthly_cohort_retention   # This complex query runs on replica, not affecting login performance   sql = &lt;&lt;-SQL    WITH cohorts AS (     SELECT       DATE_TRUNC('month', created_at) as cohort_month,      id as user_id     FROM users    ),    activities AS (     SELECT       user_id,      DATE_TRUNC('month', created_at) as activity_month     FROM orders     GROUP BY 1, 2    )    SELECT      cohorts. cohort_month,     COUNT(DISTINCT cohorts. user_id) as cohort_size,     COUNT(DISTINCT activities. user_id) as active_users,     ROUND(100. 0 * COUNT(DISTINCT activities. user_id) /         COUNT(DISTINCT cohorts. user_id), 2) as retention_rate    FROM cohorts    LEFT JOIN activities ON cohorts. user_id = activities. user_id    GROUP BY 1    ORDER BY 1   SQL      connection. exec_query(sql)  end endendNow your data team can run heavy queries without fear: 123456# This query might take 30 seconds, but won't affect your appretention_data = Analytics::UserMetrics. monthly_cohort_retention# Meanwhile, your users can still log in quicklyuser = User. find_by(email: params[:email])user. authenticate(params[:password])Monitoring Replication Health: You can’t use read replicas effectively without monitoring. Here’s a simple health check: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class ReplicaHealthCheck def self. replication_lag  result = ApplicationRecord. connected_to(role: :reading) do   ApplicationRecord. connection. execute(&lt;&lt;-SQL). first    SELECT CASE      WHEN pg_is_in_recovery() THEN       EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))     ELSE 0    END as lag_seconds   SQL  end    result['lag_seconds']. to_f. seconds end  def self. healthy?  lag = replication_lag  lag &lt; 5. seconds rescue PG::ConnectionBad  false endend# app/jobs/replica_health_monitor_job. rbclass ReplicaHealthMonitorJob &lt; ApplicationJob queue_as :monitoring  def perform  lag = ReplicaHealthCheck. replication_lag    # Report metric to your monitoring service  StatsD. gauge('database. replica. lag_seconds', lag. to_f)    # Alert if lag exceeds threshold  if lag &gt; 10. seconds   Rails. error. report(    HighReplicationLagError. new( Replication lag: #{lag} ),    severity: :warning,    context: { lag_seconds: lag. to_f }   )  end    # Schedule next check  self. class. set(wait: 30. seconds). perform_later endend# Start monitoring (in an initializer or deploy task)ReplicaHealthMonitorJob. perform_later if Rails. env. production?Common Pitfalls in Basic Setups: 1. Accidental Writes to Replicas: 1234# This will raise an errorApplicationRecord. connected_to(role: :reading) do User. create!(name:  Bob ) # ActiveRecord::ReadOnlyErrorendRails protects you by default, but always double-check your replica database user has read-only permissions: 1234-- On your PostgreSQL replicaCREATE USER replica_user WITH PASSWORD 'secret';GRANT SELECT ON ALL TABLES IN SCHEMA public TO replica_user;-- No INSERT, UPDATE, DELETE permissions2. Lazy Loading Across Connection Boundaries: 123456789101112# Be careful with lazy-loaded queriesApplicationRecord. connected_to(role: :reading) do @users = User. where(active: true) # Query not executed yetend# This executes the query outside the block, using primary instead of replica@users. each { |u| puts u. name } # Uses primary connection!# Solution: Force execution within the connection blockApplicationRecord. connected_to(role: :reading) do @users = User. where(active: true). load # Forces execution on replicaendFurther Reading: For more details on the concepts covered in this post:  Rails Documentation     Active Record Multiple Databases Guide - Official Rails guide on database configuration   Database Connection Switching - API documentation for connected_to    PostgreSQL Documentation     Streaming Replication - How PostgreSQL streaming replication works   Monitoring Replication - Using pg_stat_replication view   High Availability - PostgreSQL’s approach to HA and read replicas   What’s Next?: Make sure to check out:  Part 2 - Advanced Patterns and Gotchas Part 3 - Production ExcellenceRemember: start simple. Begin by moving just your analytics queries to replicas and expand from there. The goal is to improve performance without adding unnecessary complexity. "
    }, {
    "id": 15,
    "url": "https://prateekcodes.dev/postgresql-17-merge-with-returning-for-rails-developers/",
    "title": "PostgreSQL 17's MERGE with RETURNING: The Game-Changer Rails Developers Have Been Waiting For",
    "body": "2025/06/25 - PostgreSQL 17 introduces RETURNING support to the MERGE statement (commit c649fa24a), solving a long-standing limitation that forced developers to choose between atomic upserts and knowing what actually happened to their data. The Problem: Every Rails application eventually needs to sync data - whether from external APIs, CSV imports, or inter-service communication. The pattern is always the same: insert new records, update existing ones, and track what changed. 123456789101112131415products_from_api. each do |api_product| product = Product. find_or_initialize_by(external_id: api_product[:id]) was_new_record = product. new_record?  product. update!(  name: api_product[:name],  price: api_product[:price] )  AuditLog. create!(  action: was_new_record ? 'created' : 'updated',  record_id: product. id,  changes: product. previous_changes )endThis approach generates N+1 queries, suffers from race conditions, and requires complex logic to track operations. MERGE with RETURNING: PostgreSQL 17’s enhancement allows MERGE to return modified rows along with the operation performed: 12345678910111213141516MERGE INTO products pUSING (VALUES  ('ext_123', 'iPhone 15', 999. 99), ('ext_124', 'MacBook Pro', 2499. 99)) AS source(external_id, name, price)ON p. external_id = source. external_idWHEN MATCHED THEN UPDATE SET   name = source. name,  price = source. price,  updated_at = CURRENT_TIMESTAMPWHEN NOT MATCHED THEN INSERT (external_id, name, price, created_at, updated_at) VALUES (source. external_id, source. name, source. price,      CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)RETURNING p. *, merge_action() as action;The merge_action() function returns ‘INSERT’, ‘UPDATE’, or ‘DELETE’ for each affected row. Rails Implementation: Active Record doesn’t support MERGE natively. Here’s a practical solution: 1234567891011121314151617181920212223242526272829303132333435module MergeableRecord extend ActiveSupport::Concern class_methods do  def merge_records(records, unique_key: :id, returning: '*')   return [] if records. empty?   columns = records. first. keys   values_list = records. map do |record|     (#{columns. map { |col| connection. quote(record[col]) }. join(', ')})    end. join(', ')      update_assignments = columns. reject { |col| col == unique_key }. map do |col|     #{col} = source. #{col}    end   update_assignments &lt;&lt;  updated_at = CURRENT_TIMESTAMP       sql = &lt;&lt;-SQL    MERGE INTO #{table_name} AS target    USING (VALUES #{values_list}) AS source(#{columns. join(', ')})    ON target. #{unique_key} = source. #{unique_key}    WHEN MATCHED THEN     UPDATE SET #{update_assignments. join(', ')}    WHEN NOT MATCHED THEN     INSERT (#{columns. join(', ')}, created_at, updated_at)     VALUES (#{columns. map { |c|  source. #{c}  }. join(', ')},          CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)    RETURNING #{returning}, merge_action() as merge_action   SQL      result = connection. exec_query(sql)   result. map { |row| row. symbolize_keys }  end endendUsage: 1234567891011121314151617class Product &lt; ApplicationRecord include MergeableRecordendresults = Product. merge_records( [  { external_id: 'ext_123', name: 'iPhone 15', price: 999. 99 },  { external_id: 'ext_124', name: 'MacBook Pro', price: 2499. 99 } ], unique_key: :external_id)# results:# [#  { id: 1, external_id: 'ext_123', name: 'iPhone 15', price: 999. 99, merge_action: 'UPDATE' },#  { id: 2, external_id: 'ext_124', name: 'MacBook Pro', price: 2499. 99, merge_action: 'INSERT' }# ]Performance Comparison: Syncing 10,000 products: Traditional approach (find_or_create_by):  20,000+ queries 45 seconds Race condition proneMERGE with RETURNING:  100 queries (batched) 3 seconds Atomic operationsPractical Applications: Audit Logging: 123456789101112results = Product. merge_records(products_data, unique_key: :sku)audit_logs = results. map do |result| {  record_type: 'Product',  record_id: result[:id],  action: result[:merge_action]. downcase,  performed_at: Time. current }endAuditLog. insert_all(audit_logs)Cache Invalidation: 12345results = Product. merge_records(updated_products, unique_key: :sku)results. select { |r| r[:merge_action] == 'UPDATE' }. each do |result| Rails. cache. delete( product/#{result[:id]} )endConflict Resolution: 1234567MERGE INTO inventory iUSING new_inventory n ON i. sku = n. skuWHEN MATCHED AND i. updated_at &lt; n. updated_at THEN UPDATE SET quantity = n. quantity, updated_at = n. updated_atWHEN NOT MATCHED THEN INSERT VALUES (n. sku, n. quantity, n. updated_at)RETURNING i. *, merge_action() as action;Limitations:  Requires PostgreSQL 17+ No Active Record native support Complex MERGE conditions can impact performance Limited to single-table operationsConclusion: PostgreSQL 17’s MERGE with RETURNING eliminates the need for multiple queries and race-prone code when handling upserts. While Active Record support is pending, the patterns shown here provide immediate access to this powerful feature. For data synchronization, ETL processes, and any scenario requiring bulk upserts with operation tracking, MERGE with RETURNING transforms complex multi-query operations into single, atomic statements. "
    }, {
    "id": 16,
    "url": "https://prateekcodes.dev/rails-with-connection-better-database-connection-management/",
    "title": "Rails with_connection: The better way to manage database connections",
    "body": "2024/08/15 - Rails applications often struggle with database connection management in high-concurrency environments. The traditional ActiveRecord::Base. connection method holds connections until the end of the request cycle, potentially exhausting the connection pool. Since Rails 7. 2, there’s a better way: ActiveRecord::Base. with_connection. Before: Previously, when performing database operations, connections were held for the entire request duration: 1234567891011121314151617181920class DataImportService def import_large_dataset  # This holds a connection for the entire method execution  connection = ActiveRecord::Base. connection  csv_data. each_slice(1000) do |batch|   # Long-running operations holding the connection   process_batch(batch, connection)   # External API calls still holding the connection   notify_external_service(batch)  end end private def process_batch(batch, connection)  connection. execute( INSERT INTO imports . . .  ) endendThis approach leads to:  Connection pool exhaustion in high-traffic scenarios Reduced application throughput Database connection timeouts Poor resource utilization during I/O operationsThe Better Approach: Rails 7. 2 introduced ActiveRecord::Base. with_connection which automatically manages connection lifecycle: 12345678910111213141516171819class DataImportService def import_large_dataset  csv_data. each_slice(1000) do |batch|   # Connection is only held during database operations   ActiveRecord::Base. with_connection do |connection|    process_batch(batch, connection)   end   # Connection is released back to pool during API calls   notify_external_service(batch)  end end private def process_batch(batch, connection)  connection. execute( INSERT INTO imports . . .  ) endendThe connection is automatically:  Obtained from the pool when the block starts Yielded to the block for database operations Returned to the pool when the block completesReal-world Example: Parallel Processing: Here’s how with_connection shines in concurrent scenarios: 123456789101112131415161718192021222324252627class BulkUserUpdater def update_users_in_parallel  User. find_in_batches(batch_size: 100) do |user_batch|   # Process batches in parallel threads   user_batch. map do |user|    Thread. new do     # Each thread gets its own connection from the pool     ActiveRecord::Base. with_connection do |connection|      # Perform complex calculations      analytics_data = fetch_user_analytics(user)      # Update user with connection      connection. execute(&lt;&lt;-SQL)       UPDATE users       SET last_activity = '#{analytics_data[:last_activity]}',         engagement_score = #{analytics_data[:score]}       WHERE id = #{user. id}      SQL     end     # Connection released while sending emails     UserMailer. activity_summary(user). deliver_later    end   end. each(&amp;:join)  end endendThe Soft Deprecation: While ActiveRecord::Base. connection has been soft deprecated since Rails 7. 2, it still works without warnings by default. To see deprecation warnings, you need to explicitly configure: 1234567# config/application. rbActiveRecord. permanent_connection_checkout = :deprecated# Now you'll see:ActiveRecord::Base. connection# DEPRECATION WARNING: ActiveRecord::Base. connection is deprecated. # Use #lease_connection instead. The method has been renamed to lease_connection to better reflect that it holds the connection for the entire request duration. When to Use lease_connection: For cases requiring manual connection management, use lease_connection: 1234567891011121314151617class LongRunningJob def perform  # Manually lease a connection  connection = ActiveRecord::Base. lease_connection  begin   # Use connection for multiple operations   connection. transaction do    update_records(connection)    generate_reports(connection)   end  ensure   # Must manually release the connection   ActiveRecord::Base. connection_handler. clear_active_connections!  end endendUse lease_connection when you need:  Explicit control over connection lifecycle A connection across multiple method calls Custom connection handling logicPerformance Benefits: The new approach provides significant improvements: 12345678910111213141516171819202122232425# Benchmark comparing old vs new approachrequire 'benchmark'Benchmark. bm do |x| x. report( old approach: ) do  100. times do   conn = ActiveRecord::Base. connection   conn. execute( SELECT COUNT(*) FROM users )   sleep(0. 01) # Simulate I/O operation  end end x. report( with_connection: ) do  100. times do   ActiveRecord::Base. with_connection do |conn|    conn. execute( SELECT COUNT(*) FROM users )   end   sleep(0. 01) # Connection released during sleep  end endend#           user   system   total    real# old approach:   0. 024548  0. 024413  0. 048961 ( 1. 511756)# with_connection: 0. 010594  0. 005119  0. 015713 ( 1. 311284) The results show:  15% faster real time (1. 31s vs 1. 51s) 68% less total CPU time (0. 015s vs 0. 048s) Better connection pool utilization during I/O operationsMigration Guide: Update your existing code patterns: 1234567891011121314151617# Beforedef execute_query conn = ActiveRecord::Base. connection conn. execute( SELECT * FROM products )end# Afterdef execute_query ActiveRecord::Base. with_connection do |conn|  conn. execute( SELECT * FROM products ) endend# Or for simple cases, just use ActiveRecord methodsdef execute_query Product. allendConclusion: The new with_connection method solves a real problem - connection pool exhaustion during I/O heavy operations. It’s a simple change that makes Rails applications handle concurrent requests better without any extra configuration. References:  Pull Request #51083 introducing with_connection Pull Request #51230 deprecating connection method Rails Connection Handling Documentation Connection Pool Management Guide"
    }, {
    "id": 17,
    "url": "https://prateekcodes.dev/make-composite-index-names-better/",
    "title": "Rails 7.1 handles long auto-generated index names with a limit",
    "body": "2023/10/23 - While adding a composite index that is made up of columns with long names, the index name auto-generated by Rails can grow too long. That leads to the annoying error  Index name index_table_name_on_column1_and_column2_and_column3 on table ‘table_name’ is too long Before: After we see this error, the fix is to add a name option for adding index like so: 1add_index :opportunities, %i(manager_id operational_countries_id hospital_id opportunity_type), name:  idx_opps_on_mid_ocid_hid_otype This works great, but could be better if Rails handled that auto-magically. Rails 7. 1: This PR added a limit of 62 bytes on the auto generated index names. This is safe for MySQL, Postgres and SQLite index name limits. If the generated index name is over the limit, the naming would be done as per a shorter format. Example: 123index_table_name_on_column1_and_column2_and_column3 =&gt; Long formatix_on_column1_and_column2_and_column3_584cb5f07a =&gt; Shorter formatThe shorter format includes a hash created from the long format name to ensure uniqueness across the database. Primary formatindex_{table_name}_on_#{columns. join('_and_')} New fallback - shorter formatix_on_#{columns. join('_')}_#{OpenSSL::Digest::SHA256. hexdigest(long_name). first(10)} Digging into Mike’s PR: I’m trying something new here. I’ll go over some key aspects of the PR and how this OSS contribution fixes this problem. The aim is to simplify the solution. Changing the index name: 12345678910111213141516activerecord/lib/active_record/connection_adapters/abstract/schema_statements. rb  def index_name(table_name, options) # :nodoc: if Hash === options  if options[:column]-    index_#{table_name}_on_#{Array(options[:column]) * '_and_'} +   generate_index_name(table_name, options[:column])  elsif options[:name]   options[:name]  else   raise ArgumentError,  You must specify the index name   end else  index_name(table_name, index_name_options(options)) endend123456789101112131415161718192021222324def generate_index_name(table_name, column) name =  index_#{table_name}_on_#{Array(column) * '_and_'}  # Array(column) * '_and_' is similar to Array(column). join('_and_') # Example: index_students_on_name_and_age_and_city return name if name. bytesize &lt;= max_index_name_size # If generated `name` is within limit, return that.   hashed_identifier =  _  + OpenSSL::Digest::SHA256. hexdigest(name). first(10) name =  ix_on_#{Array(column) * '_'}  # Example: ix_on_name_age_city_0c06491783 short_limit = max_index_name_size - hashed_identifier. bytesize # Calculates the size left after counting size of the hash that has to be added at the end of the name.  # In this case 62 - 11 = 51 short_name = name. mb_chars. limit(short_limit). to_s # Limit the new short format name to the limit we have left.   #{short_name}#{hashed_identifier}  # The final name is the calculated limited short_name with the hash identifier appended to it, which would always be within the limit. enddef max_index_name_size 62endAdding migration compatibility: Any modification to Rails migration functionality must maintain compatibility with older migrations, ensuring they run the same way, just as they did when originally created in their respective Rails versions. In this case, the author has added the below override for the add_index method for the Rails version immediately preceding 7. 1. That is 7. 0. 1234def add_index(table_name, column_name, **options) options[:name] = legacy_index_name(table_name, column_name) if options[:name]. nil? superendSimply put, if name option is provided, the change in Rails 7. 1 doesn’t do anything differently, so we don’t do anything and call super. If the name has to be generated, author has implemented a compatible method legacy_index_name. Let’s dive in: 1234567891011121314151617def legacy_index_name(table_name, options) if Hash === options # In this instance, think of it as, is `options` a Hash?  if options[:column]   # If column names are provided use them to create the name as before.     index_#{table_name}_on_#{Array(options[:column]) * '_and_'}   elsif options[:name]   # If name was provided, use that instead.    options[:name]  else   raise ArgumentError,  You must specify the index name   end else  # If options isn't a Hash, call itself again with standardized options via index_name_options method  legacy_index_name(table_name, index_name_options(options)) endend1234567891011121314def index_name_options(column_names) if expression_column_name?(column_names)  # If column_names has any character except letters, numbers or underscores,  # replace them with an underscore  column_names = column_names. scan(/\w+/). join( _ ) end # Return the column_names in an expected format for legacy_index_name { column: column_names }enddef expression_column_name?(column_name)  column_name. is_a?(String) &amp;&amp; /\W/. match?(column_name)endConclusion: This was a minor pain point from the beginning and I liked when this was fixed. The deep dive is something I haven’t done before in my blogs, I’d appreciate the feedback in the comments below. "
    }, {
    "id": 18,
    "url": "https://prateekcodes.dev/the-target-blank-vulnerability/",
    "title": "The target _blank vulnerability",
    "body": "2022/02/02 - Most of us have been using target='_blank' in our links. That is a popular but dangerous practice. The problem: The page linked via target='_blank' has partial access to the linking page. That allows the linked page to open any other site on the linking page that the user came from. This could be used in a simple phishing attack. Example: A page that the user trusts has this link: 1&lt;a href= https://www. somesite. example. com  target= _blank &gt;Click here&lt;/a&gt;After clicking this link, the site somesite. example. com gets access to window. opener and they could do something like: 1window. opener. location =  https://www. unsafe. example. com/phishing ;The above code would open the unsafe site in the previous tab/window where the user came from. This is a phishing attack because the user trusts the site. The problem also presents itself when opening a page with window. open(). Solution: We can fix this by cutting the backlink (opener object) between the parent and the child pages:  Use rel= noopener noreferrer  for HTML links. 1&lt;a href= https://www. somesite. example. com  target= _blank  rel= noopener noreferrer &gt;Click here&lt;/a&gt; For pages opened with window. open(), cut the link by setting opener to null. 12let blogPage = window. open( https://www. somesite. example. com );blogPage. opener = null;More information here. Sources:  Reverse Tabnabbing - OWASP Target=”_blank” —  the most underestimated vulnerability ever"
    }, {
    "id": 19,
    "url": "https://prateekcodes.dev/pretty-print-avoids-eager-loading-relation/",
    "title": "Rails adds a limit of fetching 10 records when using pretty print",
    "body": "2022/01/31 - ActiveRecord::Relation#pretty_printis a method that pretty prints an ActiveRecord::Relation object. Example: Without pretty print: 123irb(main):002:0&gt; Post. limit(2)Post Load (2. 7ms) SELECT  posts . * FROM  posts  ORDER BY  posts .  id  ASC LIMIT $1 [[ LIMIT , 2]]=&gt; #&lt;ActiveRecord::Relation [#&lt;Post id: 1, title:  First Post , body:  First Post Body , created_at:  2022-01-18 07:30:36 , updated_at:  2022-01-18 07:30:36 &gt;, #&lt;Post id: 2, title:  Second Post , body:  Second Post Body , created_at:  2022-01-18 07:30:36 , updated_at:  2022-01-18 07:30:36 &gt;]With pretty print: 123456789101112131415irb(main):003:0&gt; pp Post. limit(2)Post Load (0. 6ms) SELECT  posts . * FROM  posts  ORDER BY  posts .  id  ASC LIMIT $1 [[ LIMIT , 2]][#&lt;Post:0x00005654ccd32d28 id: 1, title:  First Post , body:  First Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00005654ccd32b98 id: 2, title:  Second Post , body:  Second Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;]=&gt; #&lt;ActiveRecord::Relation [#&lt;Post id: 1, title:  First Post , body:  First Post Body , created_at:  2022-01-18 07:30:36 , updated_at:  2022-01-18 07:30:36 &gt;, #&lt;Post id: 2, title:  Second Post , body:  Second Post Body , created_at:  2022-01-18 07:30:36 , updated_at:  2022-01-18 07:30:36 &gt;]Before: The method works great, but since it loads all the records of the relation, it can be very slow for bigger relations. Example: 1irb(main):004:0&gt; pp Post. all # Loads all the recordsRails 7: Rails 7addsa limit of fetching upto 11 records when using pretty print much likeActiveRecord::Base#inspectif the records aren’t already loaded. Note: The 11th record is not shown. It is only loaded to determine whether there are more records to show. An ellipsis (…) is shown instead of the 11th record. Example: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364irb(main):005:0&gt; pp Post. all# Loads only 10 records and adds an ellipsis at the end if there are more recordsPost Load (0. 6ms) SELECT  posts . * FROM  posts  `/*` loading for pp `*/` ORDER BY  posts .  id  ASC LIMIT $1 [[ LIMIT , 11]][#&lt;Post:0x00007fed44c7abd0 id: 1, title:  First Post , body:  First Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c7aa68 id: 2, title:  Second Post , body:  Second Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c7a608 id: 3, title:  Second Post , body:  Second Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c7a478 id: 4, title:  Fourth Post , body:  Fourth Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c79ac8 id: 5, title:  Fifth Post , body:  Fifth Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c79398 id: 6, title:  Sixth Post , body:  Sixth Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c784c0 id: 7, title:  Seventh Post , body:  Seventh Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c73ec0 id: 8, title:  Eighth Post , body:  Eighth Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c73858 id: 9, title:  Ninth Post , body:  Ninth Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;, #&lt;Post:0x00007fed44c73538 id: 10, title:  Tenth Post , body:  Tenth Post Body , created_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00, updated_at: Tue, 18 Jan 2022 07:30:36 UTC +00:00&gt;,  . . .  ]"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});